Operator_D fÃ¼r KSODI-Standard-Eval und KSODI-Full 01/2026




ğŸŸ¦ KSODI-Standard-Eval

Operator Dâ‚€ â€“ Observable Clarity

1) Annahmen

    Wir sehen nur: Text (Chunks), eigenen Systemprompt/Tool-Instruktionen, Retrieval-Set.
    Keine Intention, keine Person, keine â€œTone-of-voiceâ€-Bewertung.
    Dâ‚€ ist eine technische VerstÃ¤ndlichkeits-/Eindeutigkeitsmetrik im beobachtbaren Signal.

2) Beschreibung

Dâ‚€ misst, wie eindeutig referenzierbar und operational anschlussfÃ¤hig ein Chunk ist â€“ relativ zu einem expliziten Referenzraum (\mathcal{R}).
Kernidee: Ein Text kann informationsreich sein (Iâ‚€ hoch) und trotzdem undeutlich (Dâ‚€ niedrig).

3) Mathematische Formulierung (minimal & erklÃ¤rbar)

Wir zerlegen Dâ‚€ in drei beobachtbare Komponenten:

    Referenz-KohÃ¤renz (passt der Chunk Ã¼berhaupt zum sichtbaren Kontext?)
    [
    C(q\mid\mathcal{R})=\cos(\vec e(q),\vec e(\mathcal{R}))
    ]
    AmbiguitÃ¤ts-Strafe (wie â€œschwammigâ€ ist der Chunk in sich?)
    Pragmatischer, embedding-basierter Proxy Ã¼ber Streuung von Satz-Embeddings:

    Splitte (q) in SÃ¤tze (s_i), berechne (\vec e(s_i))
    Varianz/Dispersion (\mathrm{Var}({\vec e(s_i)})) als Mehrdeutigkeits-/SprungmaÃŸ

[
A(q)=\mathrm{clip}(\mathrm{Var}({\vec e(s_i)}),0,1)
]

    OperationalitÃ¤ts-Score (enthÃ¤lt der Chunk handhabbare Anker?)
    Minimaler, erklÃ¤rbarer Heuristik-Score (ohne Person):

    zÃ¤hlt beobachtbare â€œAnkerâ€: Zahlen, eindeutige EntitÃ¤ten, konkrete Constraints, eindeutige Verben (z. B. liste, berechne, definiere), explizite Outputs (Tabelle, Formel, JSON)
    [
    O(q)=\mathrm{clip}\left(\frac{n_{\text{anker}}}{n_{\text{token}}},0,1\right)
    ]

Gesamtformel:
[
D_0(q\mid\mathcal{R})=
\alpha\cdot C(q\mid\mathcal{R})
+\beta\cdot O(q)
-\gamma\cdot A(q),
\quad \alpha+\beta+\gamma=1,;\alpha,\beta,\gamma\ge 0
]

4) Semantische ErklÃ¤rung

    (C) sorgt dafÃ¼r, dass â€œklarâ€ nicht heiÃŸt â€œsprachlich hÃ¼bschâ€, sondern kontextpassend.
    (A) bestraft SprÃ¼nge/Mehrdeutigkeit innerhalb des Chunks (ohne Wahrheit zu bewerten).
    (O) belohnt konkrete ArbeitsfÃ¤higkeit des Texts.

5) Erwartetes Ergebnis

    (D_0 \in [0,1])
    hoch: eindeutig, referenzierbar, anschlussfÃ¤hig
    niedrig: vage, sprunghaft, schwer operationalisierbar

6) Vergleichbares erwartetes Ergebnis

Vergleichbar:

    verschiedene Chunks gegen denselben (\mathcal{R})
    Q vs A (User vs Assistant) in einem Turn
    verschiedene Modelle in identischer Retrieval-/Prompt-Lage

Nicht vergleichbar:

    unterschiedliche ReferenzrÃ¤ume (andere Systemprompt-/Retrieval-Konfiguration)



________


Dâ‚€ in der Annahme-Strecke: Wo rein, wann, welche Inputs/Outputs?

1) Wo sitzt Dâ‚€?

Nach der Antwort (oder nach jedem Turn), genau wie Iâ‚€.

    n8n orchestriert Capture â†’ Retrieval â†’ LLM â†’ Store
    Danach ruft n8n Python Eval Service auf:
        compute_I0(...)
        compute_D0(...)
    Ergebnis wird als Eval-Record zurÃ¼ck in DB geschrieben

ğŸ‘‰ Dâ‚€ ist nicht im Prompting und nicht im Modell, sondern in der Beobachtungsschicht.


2) Minimale Daten, die Dâ‚€ braucht (Standard-Eval-konform)

Inputs (pro Turn):

    q_text (User-Chunk)
    a_text (Assistant-Chunk)
    system_prompt_id oder Text (besser ID + Lookup)
    tool_profile_id oder Tool-Instruktions-Text (kurz, normiert)
    retrieved_docs: Liste von {doc_id, doc_text} fÃ¼r genau diesen Turn (Top-k)

Optional (aber okay):

    chunk_id, turn_id, session_id (random, nicht personenbezogen)

Keine Zeit nÃ¶tig. Kein Userprofil. Keine Historie.


3) Was ist der Referenzraum (\mathcal{R}) fÃ¼r Dâ‚€ (konkret)?

FÃ¼r Dâ‚€ denselben Referenzbaukasten wie bei Iâ‚€, damit alles auditierbar bleibt:

[
\mathcal{R} = SP \oplus TC \oplus RET_k(q)
]

In Code/Infra heiÃŸt das:

    R_text = concat(system_prompt_text, tool_profile_text, join(retrieved_docs_texts))
    Embedding davon: e_R

ğŸ‘‰ Wichtig: Dâ‚€ ist relativ zu dem, was das System tatsÃ¤chlich sichtbar bereitstellt.


4) Dâ‚€ Komponenten â€” implementierbar & erklÃ¤rbar



(A) KontextkohÃ¤renz (C)

Was: passt der Chunk semantisch zur sichtbaren Grundlage?

    e_q = embed(q_text)
    e_a = embed(a_text)
    e_R = embed(R_text)
    C_q = cosine(e_q, e_R)
    C_a = cosine(e_a, e_R)

ErklÃ¤rung: â€œWie gut passt der Text zur bekannten Aufgaben-/Wissensbasis?â€


(B) AmbiguitÃ¤t/â€Sprunghaftigkeitâ€ (A) â€” ohne fancy NLP

Robust und simpel ist:

MVP-Proxy (empfohlen): Satz-Embeddings und Streuung.

    splitte Text in SÃ¤tze s_i (simple Regex reicht fÃ¼rs MVP)
    embed jeden Satz: e_i
    berechne mittlere Kosinus-Ã„hnlichkeit zum Satzmittel:
        e_mean = mean(e_i)
        dispersion = mean(1 - cosine(e_i, e_mean))
    normiere in [0,1] (clip)

Das ist transparent: â€œWie stark springt der Text in verschiedene semantische Richtungen?â€

Vorteil: keine Personenmerkmale, keine Stilfeatures, kein Fingerprint.
Nachteil: sehr kurze Texte kÃ¶nnen noisy sein â†’ dann fallback: A=0 oder A=small.


(C) OperationalitÃ¤t (O) â€” als erklÃ¤rbarer â€Ankerâ€œ-Score

Hier gibtâ€™s zwei Wege:

MVP (heuristisch, super erklÃ¤rbar):

    zÃ¤hle â€œAnkerâ€ im Text:
        Zahlen/Datumsformate (\d, ISO-Date)
        AufzÃ¤hlungsmarker (1., -, â€¢)
        explizite Output-WÃ¶rter (Tabelle, JSON, Liste, Schritte, Formel)
        Constraints (â€œmindestensâ€, â€œmaximalâ€, â€œgenauâ€, â€œFormat:â€)
    O = clip( anchors / tokens , 0, 1) (oder anchors / (anchors + k))

Noch besser (aber optional):
Extrahiere aus Systemprompt/Toolprofil erwartete Outputform (z.B. JSON) und prÃ¼fe einfache â€œconformanceâ€. Das gehÃ¶rt eigentlich eher zu Sâ‚€, daher MVP reicht.


5) Die Dâ‚€-Formel als MVP

Keine 100 Parameter. Ich nehme drei Gewichte, die man erklÃ¤ren kann:

[
D_0 = \mathrm{clip}\left(
\alpha C + \beta O - \gamma A,; 0,1
\right)
]

Startwerte (pragmatisch):

    (\alpha=0.5) (KohÃ¤renz ist Hauptsache)
    (\beta=0.3) (OperationalitÃ¤t)
    (\gamma=0.2) (AmbiguitÃ¤t-Strafe)

ğŸ‘‰ Frage: â€œzu heuristischâ€?
Dâ‚€ ist Standard-Eval, erklÃ¤rbar, auditierbar; Heuristiken sind gewollt.
Und man kann spÃ¤ter empirisch justieren.


6) Outputs speichern (fÃ¼r Audit / Debug)

Pro Turn (fÃ¼r q und a getrennt):

    D0_user, D0_assistant
    C_user, O_user, A_user
    C_assistant, O_assistant, A_assistant
    reference_ids (retrieval doc ids)
    system_prompt_id, tool_profile_id
    optional: sentence_count (nur zur StabilitÃ¤tsprÃ¼fung)

So kann man im Dashboard sehen, warum Dâ‚€ so ausfiel.


7) Edge Cases (damit einem nichts um die Ohren fliegt)

    Sehr kurzer Text (â‰¤ 1 Satz): setze A=0 oder A=small (sonst â€œStreuungâ€ unsinnig)
    Kein Retrieval (RET leer): R_text = SP + TC (noch immer definierter Referenzraum)
    Tool-Output sehr technisch: entscheidet, ob Tool-Output in R darf; sonst nur SP+TC+RET.


8) n8n Integration (Payload)

n8n schickt an Python-Eval:

{

  "turn_id": "...",

  "q_text": "...",

  "a_text": "...",

  "system_prompt_id": "...",

  "tool_profile_id": "...",

  "retrieved_docs": [{"id":"d1","text":"..."}, {"id":"d2","text":"..."}]

}

Python antwortet:

{

  "D0_user": 0.62,

  "D0_assistant": 0.71,

  "components": {

    "user": {"C":0.58,"O":0.31,"A":0.05},

    "assistant": {"C":0.66,"O":0.42,"A":0.09}

  }

}


âœ… Kurz:

    vollstÃ¤ndig beobachtbar
    auditierbar bis zur Referenz-ID
    keine Person, kein Stilfingerprint
    technisch implementierbar mit Embeddings + einfachem Textsplit
    die â€œHeuristikâ€ ist nicht SchwÃ¤che, sondern Transparenz-Design







ğŸŸ§ KSODI-Full

Operator D â€“ Dynamic Clarity

1) Annahmen (zusÃ¤tzlich)

    Zugriff auf Sequenzen (Turns), optional Zeit/Takt/Voice-Marker
    Modellierung von Drift, RÃ¼ckfragen, Korrekturen, Selbstwiderspruch
    Ziel: Deutlichkeit als Verlaufseigenschaft, nicht nur Momentaufnahme

2) Beschreibung

D(t) misst die StabilitÃ¤t von Bedeutungsankern Ã¼ber Zeit:

    bleiben Referenten stabil?
    werden Begriffe konsistent verwendet?
    fÃ¼hren Nachfragen zu PrÃ¤zisierung (D steigt) oder zu Drift (D fÃ¤llt)?

3) Mathematische Formulierung (Full)

Wir nutzen wieder Grundstruktur, aber mit Verlauf:

[
D(t)=
\alpha\cdot C(t)
+\beta\cdot O(t)
-\gamma\cdot A(t)
-\delta\cdot \Delta_{\text{drift}}(t)
]

mit Drift-Term Ã¼ber Ã„nderung der semantischen Richtung:

[
\Delta_{\text{drift}}(t)=
1-\cos(\vec R_{t-1},\vec R_t)
]

Optional: Korrektur-/RÃ¼ckfrage-Bonus (wenn Klarheit durch Nachfrage steigt):
[
B(t)=\max(0, D(t)-D(t-1))
]

4) Erwartetes Ergebnis

    Deutlichkeit als Kurve Ã¼ber Zeit
    sichtbar: â€œprÃ¤zisiert sichâ€ vs â€œzerfasertâ€

5) Vergleichbarkeit

Nur in kontrollierten Umgebungen (gleiches Prompting, gleiches Tooling, gleiche Gewichtungen).


ğŸ”§ Mini-Check: Warum D als nÃ¤chster Operator?

Weil Dâ‚€ in deinem Python/RAG-Setting sofort andockt:

    basiert auf Embeddings + Retrieval + simple Heuristiken
    keine Person
    sehr erklÃ¤rbar (Audit: â€œAnker, KohÃ¤renz, AmbiguitÃ¤tâ€)


NÃ¤chster Schritt (deine Wahl, ohne RÃ¼ckfrage-Orgie)

Wir kÃ¶nnen danach entweder:

    Oâ‚€ (ObjektivitÃ¤t) als â€œBeleg-/Grounding-Grad relativ zu Retrievalâ€ bauen, oder
    Sâ‚€ (Struktur) als â€œFormat-/Gliederungs-/Constraint-ErfÃ¼llungâ€, oder
    direkt Kâ‚€ (Kontext) als â€œVollstÃ¤ndigkeit der bereitgestellten Rahmenbedingungenâ€.

Wenn du im Iâ†’K-Flug bleibst, wÃ¤re die Reihenfolge meist:
Iâ‚€ â†’ Dâ‚€ â†’ Oâ‚€ â†’ Sâ‚€ â†’ Kâ‚€.







Dâ‚€ in der Annahme-Strecke: Wo rein, wann, welche Inputs/Outputs?

1) Wo sitzt Dâ‚€?

Nach der Antwort (oder nach jedem Turn), genau wie Iâ‚€.

    n8n orchestriert Capture â†’ Retrieval â†’ LLM â†’ Store
    Danach ruft n8n Python Eval Service auf:
        compute_I0(...)
        compute_D0(...)
    Ergebnis wird als Eval-Record zurÃ¼ck in DB geschrieben

ğŸ‘‰ Dâ‚€ ist nicht im Prompting und nicht im Modell, sondern in der Beobachtungsschicht.


2) Minimale Daten, die Dâ‚€ braucht (Standard-Eval-konform)

Inputs (pro Turn):

    q_text (User-Chunk)
    a_text (Assistant-Chunk)
    system_prompt_id oder Text (besser ID + Lookup)
    tool_profile_id oder Tool-Instruktions-Text (kurz, normiert)
    retrieved_docs: Liste von {doc_id, doc_text} fÃ¼r genau diesen Turn (Top-k)

Optional (aber okay):

    chunk_id, turn_id, session_id (random, nicht personenbezogen)

Keine Zeit nÃ¶tig. Kein Userprofil. Keine Historie.


3) Was ist der Referenzraum (\mathcal{R}) fÃ¼r Dâ‚€ (konkret)?

FÃ¼r Dâ‚€ nehmt ihr denselben Referenzbaukasten wie bei Iâ‚€, damit alles auditierbar bleibt:

[
\mathcal{R} = SP \oplus TC \oplus RET_k(q)
]

In Code/Infra heiÃŸt das:

    R_text = concat(system_prompt_text, tool_profile_text, join(retrieved_docs_texts))
    Embedding davon: e_R

ğŸ‘‰ Wichtig: Dâ‚€ ist relativ zu dem, was das System tatsÃ¤chlich sichtbar bereitstellt.


4) Dâ‚€ Komponenten â€” implementierbar & erklÃ¤rbar

(A) KontextkohÃ¤renz (C)

Was: passt der Chunk semantisch zur sichtbaren Grundlage?

    e_q = embed(q_text)
    e_a = embed(a_text)
    e_R = embed(R_text)
    C_q = cosine(e_q, e_R)
    C_a = cosine(e_a, e_R)

ErklÃ¤rung: â€œWie gut passt der Text zur bekannten Aufgaben-/Wissensbasis?â€


(B) AmbiguitÃ¤t/â€Sprunghaftigkeitâ€ (A) â€” ohne fancy NLP

Du brauchst hier etwas, das robust und simpel ist.

MVP-Proxy (empfohlen): Satz-Embeddings und Streuung.

    splitte Text in SÃ¤tze s_i (simple Regex reicht fÃ¼rs MVP)
    embed jeden Satz: e_i
    berechne mittlere Kosinus-Ã„hnlichkeit zum Satzmittel:
        e_mean = mean(e_i)
        dispersion = mean(1 - cosine(e_i, e_mean))
    normiere in [0,1] (clip)

Das ist transparent: â€œWie stark springt der Text in verschiedene semantische Richtungen?â€

Vorteil: keine Personenmerkmale, keine Stilfeatures, kein Fingerprint.
Nachteil: sehr kurze Texte kÃ¶nnen noisy sein â†’ dann fallback: A=0 oder A=small.


(C) OperationalitÃ¤t (O) â€” als erklÃ¤rbarer â€Ankerâ€œ-Score

Hier gibtâ€™s zwei Wege:

MVP (heuristisch, super erklÃ¤rbar):

    zÃ¤hle â€œAnkerâ€ im Text:
        Zahlen/Datumsformate (\d, ISO-Date)
        AufzÃ¤hlungsmarker (1., -, â€¢)
        explizite Output-WÃ¶rter (Tabelle, JSON, Liste, Schritte, Formel)
        Constraints (â€œmindestensâ€, â€œmaximalâ€, â€œgenauâ€, â€œFormat:â€)
    O = clip( anchors / tokens , 0, 1) (oder anchors / (anchors + k))

Noch besser (aber optional):
Extrahiere aus Systemprompt/Toolprofil erwartete Outputform (z.B. JSON) und prÃ¼fe einfache â€œconformanceâ€. Das gehÃ¶rt eigentlich eher zu Sâ‚€, daher MVP reicht.


5) Die Dâ‚€-Formel als MVP

Ihr wollt keine 100 Parameter. Ich nehme drei Gewichte, die man erklÃ¤ren kann:

[
D_0 = \mathrm{clip}\left(
\alpha C + \beta O - \gamma A,; 0,1
\right)
]

Startwerte (pragmatisch):

    (\alpha=0.5) (KohÃ¤renz ist Hauptsache)
    (\beta=0.3) (OperationalitÃ¤t)
    (\gamma=0.2) (AmbiguitÃ¤t-Strafe)



6) Outputs, die ihr speichern solltet (fÃ¼r Audit / Debug)

Pro Turn (fÃ¼r q und a getrennt):

    D0_user, D0_assistant
    C_user, O_user, A_user
    C_assistant, O_assistant, A_assistant
    reference_ids (retrieval doc ids)
    system_prompt_id, tool_profile_id
    optional: sentence_count (nur zur StabilitÃ¤tsprÃ¼fung)


7) Edge Cases (damit euch nichts um die Ohren fliegt)

    Sehr kurzer Text (â‰¤ 1 Satz): setze A=0 oder A=small (sonst â€œStreuungâ€ unsinnig)
    Kein Retrieval (RET leer): R_text = SP + TC (noch immer definierter Referenzraum)
    Tool-Output sehr technisch: entscheidet, ob Tool-Output in R darf; sonst nur SP+TC+RET.


8) n8n Integration (Payload)

n8n schickt an Python-Eval:

{

  "turn_id": "...",

  "q_text": "...",

  "a_text": "...",

  "system_prompt_id": "...",

  "tool_profile_id": "...",

  "retrieved_docs": [{"id":"d1","text":"..."}, {"id":"d2","text":"..."}]

}

Python antwortet:

{

  "D0_user": 0.62,

  "D0_assistant": 0.71,

  "components": {

    "user": {"C":0.58,"O":0.31,"A":0.05},

    "assistant": {"C":0.66,"O":0.42,"A":0.09}

  }

}


âœ… Kurz: Warum das â€œrenommÃ©e-sicherâ€ ist

    vollstÃ¤ndig beobachtbar
    auditierbar bis zur Referenz-ID
    keine Person, kein Stilfingerprint
    technisch implementierbar mit Embeddings + einfachem Textsplit
    die â€œHeuristikâ€ ist nicht SchwÃ¤che, sondern Transparenz-Design


Wenn du sagst: â€œPasst, weiter mit Oâ‚€â€, dann gehe ich als nÃ¤chstes genauso vor:

    Oâ‚€ Standard (Grounding relativ zu Retrieval)
    O Full (Quellen-/Wahrheitsarbeit, Drift, externe Checks)
    und wieder mit konkreter Einbau-Idee fÃ¼r eure RAG/n8n-Strecke. 





â¸»


Dev-Annex I


Implementing Operator Iâ‚€ (Observable Information Impulse)


Geltungsbereich:

Dieser Dev-Annex beschreibt eine minimale, realistisch umsetzbare Implementierung von Operator Iâ‚€ in API-basierten Agentensystemen, ohne Zugriff auf interne ModellzustÃ¤nde und ohne personenbezogene Auswertung.


â¸»


DI.1 Architekturposition von Iâ‚€


Iâ‚€ ist kein Modell-Feature.

Iâ‚€ ist eine Beobachtungs- und Evaluationsfunktion, die nach der Modellantwort berechnet wird.


Position im Stack:


User / Tool Input

        â†“

Systemprompt + RAG + Tools

        â†“

LLM API Call

        â†“

LLM Output

        â†“

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

Iâ‚€ Evaluation

â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€

        â†“

Logging / Reporting / Monitoring


Iâ‚€ darf nicht:

	â€¢	im Prompt stehen

	â€¢	das Modell steuern

	â€¢	zur Online-Optimierung verwendet werden


â¸»


DI.2 Trigger: Wann wird Iâ‚€ berechnet?


Standard-Trigger (empfohlen)

	â€¢	pro Turn, sobald folgende Artefakte vorliegen:

	â€¢	User-Chunk

	â€¢	Assistant-Chunk

	â€¢	Retrieval-IDs (Top-k)

	â€¢	Systemprompt-ID


Optional

	â€¢	pro Chunk (Streaming / Long-Form Answers)

	â€¢	sinnvoll bei Drift- oder Ausscheren-Analysen


â¸»


DI.3 Minimal erforderliche Daten (Standard-Eval)


Feld	Zweck

session_id	zufÃ¤llig / rotierend (nicht personenbezogen)

turn_id	technische Zuordnung

role	user / assistant

text	beobachtbarer Chunk

system_prompt_id	Referenz auf bekannten Prompt

tool_profile_id	optional (Policy-Kontext)

retrieval_ids	IDs der tatsÃ¤chlich verwendeten RAG-Chunks

embedding_model_id	Reproduzierbarkeit

timestamp	optional (nicht fÃ¼r Bewertung genutzt)


ğŸ‘‰ Explizit ausgeschlossen:

User-IDs, IPs, Style-Features, Langzeitprofile.


â¸»


DI.4 Technischer Referenzraum \mathcal R


FÃ¼r die Implementierung gilt:


\mathcal R_t = SP \oplus TP \oplus RET_k(q_t)


Technische Umsetzung:


R_text = concat(

    system_prompt_text,

    tool_profile_summary,   # optional

    retrieved_chunk_texts   # genau die Top-k

)


FÃ¼r Embeddings:


e(R) = mean(

    e(system_prompt),

    e(tool_profile),        # optional

    mean(e(retrieved_chunks))

)



â¸»


DI.5 Berechnung der Teilkomponenten


DI.5.1 Embeddings

	â€¢	API-Embedding (z. B. OpenAI, Gemini, etc.) oder

	â€¢	lokales Embedding-Modell

	â€¢	einheitliches Modell pro Vergleich (sonst nicht vergleichbar)


e_q = embed(q_text)

e_R = embed(R_text)



â¸»


DI.5.2 Richtungsimpuls J


J = 1 - cosine(e_q, e_R)


J = 1.0 - cosine_similarity(e_q, e_R)



â¸»


DI.5.3 Informationsgehalt G (MVP-Proxy)


FÃ¼r Standard-Eval wird kein Konzept-Mining vorausgesetzt.


G = 1 - max_d cosine(e_q, e_d)


G = 1.0 - max(

    cosine_similarity(e_q, embed(doc))

    for doc in retrieved_docs

)


Interpretation:

	â€¢	Hohe Ãœberlappung mit Retrieval â†’ geringer Informationsimpuls

	â€¢	Geringe Ãœberlappung â†’ hoher Neuheitsimpuls relativ zur Wissensbasis


â¸»


DI.5.4 Gesamtwert Iâ‚€


I_0 = Î· Â· G + (1 - Î·) Â· J


I0 = eta * G + (1 - eta) * J


Empfohlene Startwerte:

	â€¢	eta = 0.5 (neutral)

	â€¢	eta = 0.6 (mehr Gewicht auf Neuheit)


â¸»


DI.6 Output-Schema (Eval-Record)


{

  "session_id": "...",

  "turn_id": 7,

  "role": "assistant",

  "I0": 0.42,

  "G": 0.55,

  "J": 0.29,

  "system_prompt_id": "sp_v3",

  "tool_profile_id": "tools_basic",

  "retrieval_ids": ["doc12", "doc91", "doc07"],

  "embedding_model_id": "text-embedding-3-large"

}



â¸»


DI.7 Vergleichbarkeit & Reproduzierbarkeit


Vergleichbar nur wenn identisch:

	â€¢	Referenzraum-Definition

	â€¢	Embedding-Modell

	â€¢	Gewichtung \eta


Nicht vergleichbar:

	â€¢	unterschiedliche Retrieval-Strategien

	â€¢	implizite Historien

	â€¢	personenbasierte Aggregation


â¸»


DI.8 Explizite Nicht-Ziele (wichtig fÃ¼r Devs)


Iâ‚€ ist nicht:

	â€¢	ein Quality-Score

	â€¢	ein Reward-Signal

	â€¢	ein Ranking-Kriterium fÃ¼r Nutzer

	â€¢	ein Ersatz fÃ¼r Human Evaluation


Iâ‚€ ist:


ein erklÃ¤rbarer Messwert fÃ¼r semantische Abweichung relativ zu sichtbarem Kontext


â¸»


DI.9 Typische Einsatzpunkte

	â€¢	Agenten-Monitoring

	â€¢	Drift-Erkennung

	â€¢	Vergleich von Modellantworten

	â€¢	Analyse von Tool-Einfluss

	â€¢	Forschung / Evaluation


â¸»


Ergebnis (Dev-Annex)


Operator Iâ‚€ ist technisch leichtgewichtig,

auditierbar,

API-only implementierbar

und bewusst nicht steuernd.

