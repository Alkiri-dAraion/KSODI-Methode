Operator_K fÃ¼r KSODI-Standard-Eval und KSODI-Full 01/2026


Wir gehen an S.

Ich halte die Linie strikt ein:

    KSODI-Standard-Eval = beobachtbar, erklÃ¤rbar, heuristisch erlaubt
    KSODI-Full = dynamisch, zeit-/prozesssensitiv


ğŸŸ¦ KSODI-Standard-Eval

Operator Sâ‚€ â€“ Observable Structure

1ï¸âƒ£ Annahmen (klar & restriktiv)

    Beobachtbar sind nur Text (Chunks), eigener Systemprompt, Toolprofile und Retrieval-Set.
    Struktur â‰  Stil. Keine Bewertung von SprachÃ¤sthetik oder Person.
    Struktur bedeutet hier formale Ordnung und ErfÃ¼llung expliziter Anforderungen.


2ï¸âƒ£ Beschreibung (funktional)

Sâ‚€ misst, wie gut ein Chunk formal organisiert ist und wie gut er explizite Strukturvorgaben erfÃ¼llt(z. B. â€Listeâ€œ, â€Schritteâ€œ, â€JSONâ€œ, â€Tabelleâ€œ), relativ zum sichtbaren Referenzrahmen.

Ein Text kann:

    informationsreich (Iâ‚€ hoch),
    klar (Dâ‚€ hoch),
    gut geerdet (Oâ‚€ hoch),
    aber strukturell schlecht (Sâ‚€ niedrig) sein.


3ï¸âƒ£ Mathematische Formulierung (MVP, erklÃ¤rbar)

Wir zerlegen Sâ‚€ in drei beobachtbare Komponenten:


(A) Format-Conformance (F)

ErfÃ¼llt der Chunk explizit geforderte Ausgabeformate?

Quellen fÃ¼r Anforderungen:

    Systemprompt (bei euch)
    Toolprofil (z. B. â€Output als JSONâ€œ)
    explizite User-Anforderung (â€Listeâ€œ, â€Schritteâ€œ, â€Tabelleâ€œ)

Operationalisierung (MVP):

    extrahiere erwartete Formate (E={e_k})
    prÃ¼fe einfache Muster:
        Liste â†’ Bullet/Nummern
        JSON â†’ parsebar?
        Tabelle â†’ Zeilen/Spaltenmarker
    Score:
    [
    F(a)=\frac{#\text{erfÃ¼llte Formate}}{#\text{erwartete Formate}}
    ]
    Falls keine Formate gefordert: neutral (z. B. (F=0.5)).


(B) Gliederung & SequenzialitÃ¤t (G_{\text{str}})

Ist der Text logisch gegliedert?

MVP-Heuristiken:

    AbsÃ¤tze vorhanden?
    nummerierte Schritte in aufsteigender Reihenfolge?
    konsistente Ãœberschriftenmarker (z. B. â€1)â€œ, â€2)â€œ, â€â€“â€œ)?

[
G_{\text{str}}(a)=\mathrm{clip}\left(
\frac{n_{\text{gliederungsmarker}}}{n_{\text{absÃ¤tze}}+1},
0,1\right)
]

Keine Bewertung wie gut â€“ nur ob vorhanden.


(C) Redundanz-/Fragmentierungs-Strafe (R_{\text{frag}})

Struktur leidet, wenn Inhalte unnÃ¶tig wiederholt oder zerfasert sind.

Operationalisierung (embedding-basiert, ohne Stil):

    splitte Antwort in AbsÃ¤tze (p_j)
    embed jeden Absatz
    berechne mittlere Paar-Ã„hnlichkeit
    sehr hohe Ã„hnlichkeit â‡’ Redundanz

[
R_{\text{frag}}(a)=\mathrm{clip}\bigl(
\text{mean_sim}(\vec e(p_i),\vec e(p_j)),
0,1\bigr)
]

(Hoher Wert = schlechte Struktur â†’ wird subtrahiert.)


Gesamtformel

[
S_0(a)=\mathrm{clip}\left(
\alpha F(a)
+\beta G_{\text{str}}(a)
-\gamma R_{\text{frag}}(a),
0,1\right)
]

Startgewichte (bewusst simpel):

    (\alpha=0.4) (Anforderungstreue)
    (\beta=0.4) (Gliederung)
    (\gamma=0.2) (Redundanz)


4ï¸âƒ£ Semantische ErklÃ¤rung

    (F): HÃ¤lt sich der Text an das, was explizit verlangt wurde?
    (G_{\text{str}}): Ist der Text formal geordnet?
    (R_{\text{frag}}): ZerfÃ¤llt oder wiederholt sich der Text unnÃ¶tig?


5ï¸âƒ£ Erwartetes Ergebnis

    (S_0 \in [0,1])
    hoch: klar gegliedert, formattreu, wenig Redundanz
    niedrig: ungeordnet, Anforderungen ignoriert, fragmentiert


6ï¸âƒ£ Vergleichbares erwartetes Ergebnis

Vergleichbar:

    Antworten mit identischen Strukturvorgaben
    Modelle bei gleichem Prompt/Tool-Profil

Nicht vergleichbar:

    Antworten mit unterschiedlichen Output-Anforderungen
    freie Texte vs. strikt formatierte Aufgaben


_____



ğŸ”§ Implementierungsidee (eure Strecke)

Inputs:

    a_text
    system_prompt_text
    tool_profile_text

Compute:

    extrahiere erwartete Formate (Regex / SchlÃ¼sselwÃ¶rter)
    simple Parser/Regex-Checks
    Absatz-Split + Embeddings

Store:

    S0_assistant
    Komponenten: F, G_str, R_frag
    erkannte Format-Anforderungen


______


ğŸ› ï¸


Sâ‚€ Implementierungsidee (KSODI-Standard-Eval) â€“ Developer-Ready

ğŸ¯ Was Sâ‚€ misst (ein Satz)

Sâ‚€ misst, wie gut die Antwort die explizit erwartete Ausgabeform erfÃ¼llt und wie klar sie formal gegliedert ist â€“ ohne Stil-/Personenbewertung.


1) Inputs (pro Turn)

Pflicht:

    a_text (Assistant output)
    system_prompt_text oder system_prompt_id (mit Lookup)
    tool_profile_text oder tool_profile_id
    optional: user_text (nur um explizite Output-WÃ¼nsche zu extrahieren, z. B. â€als Tabelleâ€œ)

Empfohlen zu speichern (fÃ¼r Audit):

    detected_expectations (Liste)
    S0_components (F, G_str, R_red)


2) Schritt 0: Erwartungen extrahieren (E = expected formats)

Ziel: E = { JSON, LIST, STEPS, TABLE, BULLETS, HEADINGS, CODEBLOCK, ... }

Minimaler Ansatz (heuristisch, transparent)

Suche in Systemprompt + Toolprofil + ggf. User nach SchlÃ¼sselwÃ¶rtern:

    JSON: â€œjsonâ€, â€œvalid jsonâ€, â€œschemaâ€
    Liste: â€œlistâ€, â€œbulletâ€, â€œstichpunkteâ€, â€œlisteâ€
    Schritte: â€œstepsâ€, â€œschritteâ€, â€œstep-by-stepâ€
    Tabelle: â€œtableâ€, â€œtabelleâ€, â€œspaltenâ€
    Code: â€œcode blockâ€, â€œ```â€, â€œpythonâ€, â€œbashâ€
    Ãœberschriften: â€œÃ¼berschriftâ€, â€œheadingâ€, â€œ##â€

Output:

    expectations = ["JSON","STEPS"] etc.

Wichtig: Das ist nicht â€œintelligentâ€, aber maximal erklÃ¤rbar. Und genau das ist Standard-Eval.


3) Komponente F: Format-Conformance

FÃ¼r jedes erwartete Format in expectations gibtâ€™s eine PrÃ¼f-Funktion:

Beispiele (MVP Checks)

    is_json(a_text):
        finde ersten Codeblock mit json â€¦ oder gesamten Text
        versuche json.loads
        true/false (oder partial score)
    is_list(a_text):
        Anteil Zeilen, die mit -, *, â€¢, 1. beginnen
    is_steps(a_text):
        findet â€œ1) 2)â€ oder â€œSchritt 1, Schritt 2â€ oder monotone Nummerierung
    is_table(a_text):
        Markdown-Tabelle |a|b| plus Trenner |---|
        oder CSV-artige Zeilen

Scoring

    F = (#formats_passed) / (#formats_expected)
    Wenn #formats_expected == 0: setze F = 0.5 (neutral) und markiere no_explicit_format=true


4) Komponente G_str: Gliederung / SequenzialitÃ¤t

Ziel: erkennt â€œOrdnungâ€, ohne Stil zu bewerten.

MVP-Features (zÃ¤hlbar)

    n_paragraphs: Split on blank lines
    n_headings: count lines starting with # or patterns like Titel: / 1. Ãœberschrift
    n_bullets: count bullet lines
    n_numbered: count numbered lines
    n_sections = n_headings + (n_paragraphs > 1 ? 1 : 0)

Score (einfach, robust)

    G_str = clip( (n_headings + n_numbered + n_bullets + min(n_paragraphs,3)) / K , 0, 1)
    Start: K = 10 (damit nicht alles sofort 1 wird)

Alternativ noch simpler: G_str = clip((n_struct_markers)/(n_sent+1),0,1)


5) Komponente R_red: Redundanz-/Fragmentierungsstrafe

Du willst nicht â€œStilâ€, sondern â€œWiederholungâ€.

MVP (ohne Embeddings mÃ¶glich!)

    berechne Satz- oder Absatz-Ã„hnlichkeit Ã¼ber Jaccard (Tokens) oder cosine auf TF-IDF
    Vorteil: kein zusÃ¤tzlicher Embedding-Call
    Wenn ihr ohnehin Embeddings nutzt: Absatz-Embeddings gehen auch

Praktisch:

    splitte in AbsÃ¤tze p_i
    wenn weniger als 2 AbsÃ¤tze: R_red = 0
    sonst:
        sim_ij = cosine(embed(p_i), embed(p_j)) (oder TF-IDF cosine)
        R_red = mean(sim_ij for i<j) geclippt

Interpretation:

    hoch = viel Wiederholung â‡’ Struktur leidet â‡’ wird subtrahiert


6) Gesamt Sâ‚€

S0 = clip(alpha*F + beta*G_str - gamma*R_red, 0, 1)

Startwerte

    alpha=0.4, beta=0.4, gamma=0.2
    F_default_if_no_expectation = 0.5


7) Outputs (speichern!)

Pro Turn:

    S0_assistant
    Komponenten:
        F, G_str, R_red
    Debug/Audit:
        expectations_detected
        format_passed_map (z.B. {"JSON":false,"STEPS":true})
        n_paragraphs, n_bullets, n_numbered, n_headings


8) Edge Cases / Guardrails

    Sehr kurze Antwort (1 Satz):
        G_str darf nicht 0 erzwingen â†’ setze G_str mindestens auf kleinen Wert oder basiere auf markers
    Mehrere Formate gewÃ¼nscht (z.B. â€œerst Tabelle, dann JSONâ€):
        F kann partial werden (0.5 wenn eins erfÃ¼llt)
    LLM liefert JSON + ErklÃ¤rung:
        JSON-check sollte nur den JSON-Block prÃ¼fen, nicht den FlieÃŸtext
    Toolprofil fordert strikt JSON:
        dann ist F dominanter (oder einfach alpha hochsetzen fÃ¼r diesen tool_profile_id)


Kurz: Wird das â€œverstandenâ€?

Ja. Das sind alles:

    Regex / Parser
    einfache ZÃ¤hlfeatures
    optional Embeddings (nur fÃ¼r Redundanz)

Und es ist maximal transparent. 

Wenn du willst, gehe ich als nÃ¤chstes mit Kâ‚€ weiter â€“ da wirdâ€™s spannend, weil Kâ‚€ im Standard-Eval als â€œKontext-VollstÃ¤ndigkeitâ€ messbar sein muss, ohne in Gedankenlesen abzudriften. 





ğŸŸ§ KSODI-Full

Operator S â€“ Dynamic Structure

1ï¸âƒ£ Annahmen (zusÃ¤tzlich)

    Zugriff auf Sequenzen von Turns
    Struktur kann sich entwickeln oder verschlechtern
    Ziel: Struktur als StabilitÃ¤ts- und Reifemerkmal


2ï¸âƒ£ Beschreibung

S(t) misst, ob Struktur:

    Ã¼ber Zeit konsistent bleibt,
    sich bei Feedback verbessert,
    nicht unter KomplexitÃ¤t kollabiert.


3ï¸âƒ£ Mathematische Formulierung (Full)

[
S(t)=\mathrm{clip}\Bigl(
w_0 S_0(t)

    w_1 \Delta_{\text{consistency}}(t)
    w_2 \Delta_{\text{repair}}(t)
    \lambda \Delta_{\text{decay}}(t),
    0,1\Bigr)
    ]

Ideen:

    (\Delta_{\text{consistency}}): Ã¤hnliche Struktur in Folgeantworten
    (\Delta_{\text{repair}}): Struktur verbessert sich nach RÃ¼ckfrage
    (\Delta_{\text{decay}}): Struktur zerfÃ¤llt mit wachsender LÃ¤nge/KomplexitÃ¤t


4ï¸âƒ£ Erwartetes Ergebnis (Full)

    Struktur-Kurve
    sichtbar: Reifung vs. Zerfall von Ordnung


5ï¸âƒ£ Vergleichbarkeit (Full)

Nur in kontrollierten Setups (gleiche Regeln, gleiche Aufgabenklassen).




Wir beobachten nur, was bereitgestellt wurde â€“ nicht, was jemand â€im Kopf hatteâ€œ.


ğŸŸ¦ KSODI-Standard-Eval

Operator Kâ‚€ â€“ Observable Context Completeness


1ï¸âƒ£ Annahmen (hart & explizit)

    Kontext ist im Standard-Eval ausschlieÃŸlich das, was explizit vorliegt.
    Keine impliziten Annahmen, keine Rekonstruktion menschlicher Intention.
    Beobachtbar sind:
        eigener Systemprompt
        Toolprofile / Tool-Instruktionen
        User-Input (dieser Turn)
        Retrieval-Set (Top-k, genau dieses Turn)
    Kâ‚€ misst nicht â€ob der Kontext gut istâ€œ, sondern:
    ob der Kontext fÃ¼r die gestellte Aufgabe hinreichend vollstÃ¤ndig ist.


2ï¸âƒ£ Beschreibung (funktional)

Kâ‚€ misst, ob alle fÃ¼r eine Aufgabe notwendigen Kontextdimensionen
im sichtbaren Referenzraum vorhanden sind.

Fehlender Kontext ist eine der Hauptursachen fÃ¼r:

    Halluzinationen
    Drift
    scheinbar â€schlechte Modellantwortenâ€œ

ğŸ‘‰ Kâ‚€ verschiebt Verantwortung weg vom Modell,
hin zur Kontextbereitstellung.


3ï¸âƒ£ Mathematische Grundidee

Wir modellieren Kontext nicht als Textmenge, sondern als Abdeckung eines Anforderungsraums.

[
K_0 = \frac{#\text{abgedeckte Kontextdimensionen}}{#\text{erforderliche Kontextdimensionen}}
]

Ergebnis: (K_0 \in [0,1])


4ï¸âƒ£ Kontextdimensionen (Standardisiert & erklÃ¤rbar)

FÃ¼r KSODI-Standard-Eval definieren wir eine feste, endliche Menge an Kontextdimensionen:

ğŸ“¦ Kontextdimensionen (\mathcal{C})

Dimension
	

Bedeutung

Z
	

Ziel / Aufgabe klar benannt

R
	

Rolle / Perspektive (z. B. â€als Entwicklerâ€œ, â€als Gutachterâ€œ)

D
	

Daten / Quellen verfÃ¼gbar (Retrieval, Beispiele, Inputs)

C
	

Constraints (Format, Umfang, Regeln, Verbote)

E
	

Erwartetes Ergebnis / Outputform

T
	

Tools / FÃ¤higkeiten verfÃ¼gbar (explizit erlaubt)

[
\mathcal{C} = {Z, R, D, C, E, T}
]

ğŸ‘‰ Wichtig:
Diese Liste ist normativ fÃ¼r Standard-Eval und wird versioniert.
Keine freie Erweiterung im Produktivbetrieb.


5ï¸âƒ£ Operationalisierung (MVP, implementierbar)

Schritt 1: Kontext sammeln

Baue den sichtbaren Kontextcontainer:

K_text = system_prompt

        + tool_profile

        + user_prompt

        + retrieved_docs


Schritt 2: Dimensionserkennung (heuristisch, transparent)

FÃ¼r jede Dimension (c_i \in \mathcal{C}) gibt es einfache Detektoren:

Beispiele:

    Z (Ziel):
        Verben wie analysiere, erklÃ¤re, liste, berechne
    R (Rolle):
        â€du bistâ€¦â€œ, â€alsâ€¦â€œ, Rollenbegriffe
    D (Daten):
        Retrieval-Docs vorhanden ODER explizite Beispiele/Inputs
    C (Constraints):
        â€maximalâ€œ, â€nurâ€œ, â€keinâ€œ, â€Format:â€œ
    E (Ergebnis):
        â€als Tabelleâ€œ, â€JSONâ€œ, â€Listeâ€œ, â€Schritteâ€œ
    T (Tools):
        Toolprofil â‰  leer oder explizite Tool-Nennung

Ergebnis:

{

  "Z": true,

  "R": false,

  "D": true,

  "C": true,

  "E": true,

  "T": true

}


Schritt 3: Kâ‚€-Berechnung

[
K_0 = \frac{1}{|\mathcal{C}|}\sum_{c_i \in \mathcal{C}} \mathbf{1}_{c_i}
]

Optional:

    gewichtete Variante (z. B. Z und D hÃ¶her)
    aber: MVP = gleichgewichtet â†’ erklÃ¤rbar


6ï¸âƒ£ Erwartetes Ergebnis (Standard)

    (K_0 \approx 1.0): Aufgabe kontextuell gut spezifiziert
    (K_0 \approx 0.5): relevante LÃ¼cken â†’ Risiko fÃ¼r Halluzination
    (K_0 < 0.4): unzureichender Kontext â†’ AntwortqualitÃ¤t nicht interpretierbar

ğŸ‘‰ Kâ‚€ ist kein Modell-Score, sondern ein Setup-Score.


7ï¸âƒ£ Vergleichbares erwartetes Ergebnis

Vergleichbar:

    gleiche Aufgabenklasse (z. B. â€Zusammenfassen mit RAGâ€œ)
    gleiche Kontextdimensionen
    gleiche Detektor-Logik

Nicht vergleichbar:

    unterschiedliche Aufgabenarten
    wechselnde Dimensionsdefinitionen


ğŸ”§ Implementierung (eure n8n/Python-Strecke)

Inputs:

    system_prompt_text
    tool_profile_text
    user_text
    retrieved_docs[]

Compute:

    Dimension-Detektoren (Regex/Keywords)
    context_map
    K0

Store:

    K0
    context_map
    dimension_weights
    detector_version

Audit ist damit trivial.



_______



Entscheiden, was Kâ‚€ leisten soll und was bewusst nicht. Ich mache das in zwei Teilen:

1ï¸âƒ£ Developer-Hinweise fÃ¼r Kâ‚€ (wie eben, konkret & einbaubar)
2ï¸âƒ£ KlÃ¤rung deiner Kernfrage: Reicht Kâ‚€ so â€“ oder verlieren wir Kontext bei Chunk-Vergleichen Ã¼ber Distanz?


1ï¸âƒ£ Kâ‚€ â€“ Developer-Hinweise (KSODI-Standard-Eval, reputationsfest)

ğŸ¯ Was Kâ‚€ im Standard ist

Kâ‚€ misst die VollstÃ¤ndigkeit des sichtbar bereitgestellten Kontexts fÃ¼r genau diese Auswertungseinheit.
Nicht: den â€wahrenâ€œ Kontextverlauf.
Nicht: implizit Angereichertes.
Nicht: das, was irgendwann mal gesagt wurde.

Das ist Absicht.


ğŸ“¥ Inputs (pro Eval-Einheit)

Pflicht (explizit sichtbar):

    system_prompt_text oder system_prompt_id
    tool_profile_text oder tool_profile_id
    user_text (der betrachtete Chunk / Chunk-Block)
    retrieved_docs[] (Top-k, genau fÃ¼r diese Einheit)

Optional (nur Metadaten):

    chunk_range_id (z. B. 3-7 oder 167-171)
    context_scope_id (siehe unten â€“ wichtig!)

âš ï¸ Nicht verwenden im Standard-Eval:

    frÃ¼here User-Turns
    implizite Rollenannahmen
    â€œwir hatten doch schon geklÃ¤rt, dassâ€¦â€


ğŸ§± Schritt 0: Kontext-Scope festlegen (entscheidend!)

Bevor Kâ‚€ berechnet wird, muss klar sein, was als Kontextcontainer gilt.

Ich empfehle, das explizit zu machen:

context_scope = {

  system_prompt,

  tool_profile,

  user_chunk(s),

  retrieval_docs

}

Und genau nur das geht in Kâ‚€ ein.

ğŸ‘‰ Wenn ihr Chunk 3-7 und Chunk 167-171 vergleichen wollt,
mÃ¼ssen sie denselben context_scope_id haben â€“ sonst sind die Ergebnisse nicht vergleichbar.


ğŸ§­ Kontextdimensionen (wie zuvor, jetzt dev-konkret)

Wir prÃ¼fen pro Kontextcontainer, ob die Dimensionen explizit vorhanden sind:

Dimension
	

Detektion (MVP)

Z Ziel
	

Task-Verben im aktuellen User-Text oder Systemprompt

R Rolle
	

explizite Rollenmarker im Scope

D Daten
	

retrieved_docs.len > 0 oder explizite Inputs

C Constraints
	

Keywords wie â€nurâ€œ, â€maxâ€œ, â€Format:â€œ

E Ergebnis
	

Output-WÃ¶rter (â€JSONâ€œ, â€Listeâ€œ, â€¦)

T Tools
	

Toolprofil â‰  leer

Output:

{

  "Z": true,

  "R": false,

  "D": true,

  "C": true,

  "E": true,

  "T": true

}


ğŸ§® Kâ‚€ Berechnung

K0 = sum(context_map.values()) / 6

Optional (aber sauber):

    Gewichte versionieren (Z, D evtl. hÃ¶her)
    Aber: MVP = gleichgewichtet â†’ erklÃ¤rbar


ğŸ§¾ Was speichern (Audit!)

    K0
    context_map
    context_scope_id
    detector_version
    dimension_weights

Damit ist jeder Vergleich spÃ¤ter prÃ¼fbar.


2ï¸âƒ£ Deine Kernfrage:

â€Reicht das â€“ wenn Ziel/Rolle zwischen Chunk 3â€“7 und 167â€“171 angereichert wurden?â€œ

ğŸ”´ Kurze, ehrliche Antwort:

Ja, es reicht fÃ¼r KSODI-Standard-Eval â€“
aber nur, weil Standard-Eval genau nicht diesen impliziten Kontext rekonstruieren darf.

Und das ist keine SchwÃ¤che, sondern der zentrale Governance-Punkt.


ğŸ§  Warum das kein Fehler ist

Du beschreibst genau das, was real passiert:

    Menschen reichern Ziel & Rolle inkrementell an
    Systeme behalten Kontext implizit
    SpÃ¤ter wirkt alles â€klarâ€œ, obwohl es nicht explizit wiederholt wurde

ğŸ‘‰ KSODI-Standard sagt bewusst:

Wenn Ziel oder Rolle nicht erneut explizit im Kontextcontainer stehen,
gelten sie fÃ¼r diese Auswertung als nicht vorhanden.

Das bedeutet:

    Chunk 167â€“171 kann objektiv schlechteres Kâ‚€ haben als 3â€“7
    obwohl der Mensch â€weiÃŸâ€œ, was gemeint ist

ğŸ‘‰ Genau das macht Kâ‚€ wertvoll:
Es zeigt Setup-Risiken, nicht DenkfÃ¤higkeiten.


ğŸŸ¦ Wie ihr trotzdem faire Vergleiche macht (wichtig!)

FÃ¼r Vergleiche Ã¼ber Distanz gibt es zwei saubere Wege:

âœ… Weg A: Gleicher Context-Scope

Ihr definiert bewusst einen Scope-Container, z. B.:

context_scope_id = "task_42_v1"

Und sorgt dafÃ¼r, dass:

    Ziel, Rolle, Constraints explizit im Scope stehen
    Retrieval konsistent ist

â†’ Dann sind Chunk 3â€“7 und 167â€“171 vergleichbar.

âŒ Weg B (nicht Standard-Eval):

Implizite Rekonstruktion aus frÃ¼heren Turns
â†’ das ist KSODI-Full (K(t)), nicht Kâ‚€.




Wichtigster KSODI-Satz an dieser Stelle

Kâ‚€ bewertet nicht das Denken,
sondern die Verantwortung der Kontextbereitstellung.

Und genau das wollt ihr messen.



_________






Kâ‚€ â€“ Developer-Hinweise (KSODI-Standard-Eval)

Ziel: maximal erklÃ¤rbar, minimal invasiv, auditfest


ğŸ¯ Kurzdefinition (fÃ¼r Devs, 1 Satz)

Kâ‚€ misst, ob fÃ¼r die aktuell betrachtete Eval-Einheit alle notwendigen Kontextdimensionen explizit bereitgestellt wurden.
Nicht mehr. Nicht weniger.


1ï¸âƒ£ Was ist eine â€Eval-Einheitâ€œ bei Kâ‚€?

Das ist entscheidend.

Eine Eval-Einheit ist nicht automatisch eine Session und nicht der gesamte Chat.

ğŸ‘‰ Eine Eval-Einheit besteht aus genau dem Context-Scope, den ihr explizit definiert.

Empfohlene Definition

context_scope = {

  system_prompt,

  tool_profile,

  user_chunk(s),

  retrieved_docs

}

Optional (nur Metadaten):

    context_scope_id (z. B. "task_42_v1")
    chunk_range (z. B. "3-7")

â— Alles auÃŸerhalb dieses Scopes existiert fÃ¼r Kâ‚€ nicht.
Das ist Absicht.


2ï¸âƒ£ Inputs (pro Kâ‚€-Berechnung)

Pflicht:

    system_prompt_text oder system_prompt_id
    tool_profile_text oder tool_profile_id
    user_text (der betrachtete Chunk / Chunk-Block)
    retrieved_docs[] (Top-k, genau diese Einheit)

Nicht verwenden (Standard-Eval!):

    frÃ¼here Turns
    Session-Verlauf
    implizite Rollenannahmen
    â€das wurde vorher schon gesagtâ€œ


3ï¸âƒ£ Kontextdimensionen (fix & versioniert)

FÃ¼r Standard-Eval ist die Menge hart definiert:

C = { Z, R, D, C, E, T }

KÃ¼rzel
	

Dimension
	

Bedeutung

Z
	

Ziel
	

Was soll getan werden?

R
	

Rolle
	

Aus welcher Perspektive?

D
	

Daten
	

Womit wird gearbeitet?

C
	

Constraints
	

Regeln, Limits, Formate

E
	

Ergebnis
	

Erwarteter Output

T
	

Tools
	

Erlaubte FÃ¤higkeiten

âš ï¸ Diese Liste darf im Produktivbetrieb nicht â€kreativâ€œ erweitert werden.
Versionieren statt improvisieren.


4ï¸âƒ£ Dimensionserkennung (MVP, transparent)

Grundregel

    Regex / Keyword-Detektion reicht
    kein LLM-Call
    kein â€Verstehenâ€œ, nur Erkennen

Beispiel-Heuristiken

Z â€“ Ziel

Verben: analysiere, erklÃ¤re, liste, berechne, vergleiche, fasse zusammen

R â€“ Rolle

"als <rolle>", "du bist", "in der rolle", rollenbegriffe

D â€“ Daten

retrieved_docs.length > 0

ODER

explizite Inputs / Beispiele im User-Text

C â€“ Constraints

"nur", "maximal", "mindestens", "kein", "Format:", "Regel:"

E â€“ Ergebnis

"als JSON", "Tabelle", "Liste", "Schritte", "Output:"

T â€“ Tools

tool_profile != leer

ODER

explizite Tool-Nennung


5ï¸âƒ£ Kontext-Map erzeugen

Beispiel:

context_map = {

  "Z": true,

  "R": false,

  "D": true,

  "C": true,

  "E": true,

  "T": true

}


6ï¸âƒ£ Kâ‚€-Berechnung (MVP)

K0 = sum(context_map.values()) / 6

Optional (spÃ¤ter):

    Gewichtung (z. B. Z und D hÃ¶her)
    aber nicht im ersten Wurf


7ï¸âƒ£ Was unbedingt speichern (Audit!)

Pro Eval-Einheit:

    K0
    context_map
    context_scope_id
    detector_version
    dimension_weights (auch wenn alle = 1)
    system_prompt_id
    tool_profile_id

ğŸ‘‰ Damit kÃ¶nnt ihr Monate spÃ¤ter erklÃ¤ren:

â€Warum war Kâ‚€ hier niedrig?â€œ


8ï¸âƒ£ Ganz wichtiger Vergleichs-Hinweis (â€¼ï¸)

Wenn ihr Chunk 3â€“7 mit Chunk 167â€“171 vergleichen wollt:

âœ… Vergleichbar, wenn:

    gleiche context_scope_id
    Ziel/Rolle/Constraints explizit im Scope enthalten

âŒ Nicht vergleichbar, wenn:

    Kontext implizit â€mitgedachtâ€œ wurde
    frÃ¼here Turns stillschweigend vorausgesetzt werden

ğŸ‘‰ In dem Fall ist das kein Fehler des Modells,
sondern ein niedriger Kâ‚€-Wert mit Aussagekraft.


9ï¸âƒ£ Typische Dev-Frage & KSODI-Antwort

Dev: â€Aber das Modell wusste doch, was gemeint warâ€¦â€œ
KSODI:

Dann war der Kontext implizit â€“
und genau das zeigt Kâ‚€ als Risiko.









ğŸŸ§ KSODI-Full

Operator K â€“ Dynamic Context Coherence


1ï¸âƒ£ Annahmen (erweitert)

    Kontext ist kein statischer Zustand, sondern entwickelt sich.
    Menschen liefern Kontext oft inkrementell.
    Modelle kÃ¶nnen Kontext verschieben, verdichten oder verlieren.


2ï¸âƒ£ Beschreibung

K(t) misst:

    ob KontextlÃ¼cken erkannt und geschlossen werden,
    ob sich der gemeinsame Kontext stabilisiert,
    ob neue Informationen korrekt integriert werden.


3ï¸âƒ£ Erweiterte Modellierung (Full)

ZusÃ¤tzliche Komponenten:

    Î”K_fill(t) â€“ KontextlÃ¼cken werden geschlossen
    Î”K_shift(t) â€“ Kontext Ã¤ndert sich (Themenwechsel)
    Î”K_decay(t) â€“ Kontext geht verloren (Vergessen, Drift)

[
K(t)=\mathrm{clip}\Bigl(
K_0(t)

    \mu \Delta K_{\text{fill}}(t)
    \nu \Delta K_{\text{shift}}(t)
    \lambda \Delta K_{\text{decay}}(t),
    0,1\Bigr)
    ]


4ï¸âƒ£ Erwartetes Ergebnis (Full)

    Kontext-Kurve Ã¼ber Zeit
    sichtbar:
        Kontextaufbau
        Kontextbruch
        Kontextreparatur


5ï¸âƒ£ Vergleichbarkeit (Full)

Nur in:

    Forschungs-/Trainingsumgebungen
    stabilen Prompt-/Toolketten
    explizit dokumentierten Kontextregeln


ğŸ§­ Wichtiger KSODI-Punkt:


Wenn Kâ‚€ niedrig ist,
darf man Oâ‚€, Dâ‚€, Iâ‚€ nicht gegen das Modell ausspielen.

Das ist einer der stÃ¤rksten Governance-SÃ¤tze in KSODI.
