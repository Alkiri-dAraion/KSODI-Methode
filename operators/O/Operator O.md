Operator_O fÃ¼r KSODI-Standard-Eval und KSODI-Full 01/2026




ğŸŸ¦ KSODI-Standard-Eval

Operator Oâ‚€ â€“ Observable Objectivity (Grounding relativ zu Retrieval)

1ï¸âƒ£ Annahmen (explizit & restriktiv)

    LLM ist Black Box; Hersteller-Prompts unbekannt.
    Beobachtbar sind: User/Assistant-Text, eigener Systemprompt, Toolprofile, Retrieval-Set (Top-k), Tool-Outputs (falls im Kontext genutzt).
    Keine WahrheitsprÃ¼fung in der Welt (keine Web-Recherche als Bestandteil von Oâ‚€).
    ObjektivitÃ¤t bedeutet hier nicht â€œwahrâ€, sondern:
    â€Aussagen sind im sichtbaren Kontext belegbar / ableitbarâ€œ.

2ï¸âƒ£ Beschreibung (funktional)

Oâ‚€ misst, wie stark ein Chunk (typisch: die Assistant-Antwort) auf beobachtbaren, bereitgestellten Quellen basiert:

    passt semantisch zu Retrieval?
    enthÃ¤lt nachvollziehbare Ableitung?
    vermeidet â€œfreieâ€ Behauptungen ohne StÃ¼tze?

Oâ‚€ ist damit ein Grounding-Score â€“ kein Wahrheits- oder QualitÃ¤tsurteil.

3ï¸âƒ£ Mathematische Formulierung (MVP, erklÃ¤rbar)

Wir zerlegen Oâ‚€ in drei Teile:

(A) Retrieval-Alignment (A_{\text{ret}})

Wie gut passt der Chunk zu den gelieferten Retrieval-Dokumenten?

    embed Antwort (a) â†’ (\vec e(a))
    embed jedes Retrieval-Doc (d_i) â†’ (\vec e(d_i))

[
A_{\text{ret}}(a)=\max_i \cos(\vec e(a), \vec e(d_i))
]

Interpretation:
hoch = Antwort ist inhaltlich eng am Retrieval,
niedrig = Antwort driftet weg.


(B) Attribution/Traceability (T) (transparent, heuristisch)

Wie stark zeigt der Text Beleg-/Ableitungssignale?

MVP-Heuristik (ohne Fingerprint, ohne Stilwertung): ZÃ¤hle Attributionsmarker:

    â€lautâ€œ, â€gemÃ¤ÃŸâ€œ, â€im Dokumentâ€œ, â€in Abschnittâ€œ, â€Quelleâ€œ, â€sieheâ€œ, â€steht inâ€¦â€œ
    Zitate/Referenzen (z. B. â€â€¦â€œ oder [1], (Kap. 2))
    explizite Verweisstrukturen (â€œwie obenâ€, â€œaus den Punkten 1â€“3 folgtâ€)

[
T(a)=\mathrm{clip}\left(\frac{n_{\text{attrib}}}{n_{\text{sent}}+1}, 0, 1\right)
]

Wichtig: T ist kein Sprachstil-Score, sondern â€œgibt der Text eine Spur?â€.


(C) Unsupported-Claim Penalty (U) (keine WeltprÃ¼fung, nur KontextprÃ¼fung)

Wir bestrafen behauptungsartige SÃ¤tze, die weder

    semantisch im Retrieval verankert sind noch
    eine Ableitungsspur enthalten.

Operationalisierung (MVP):

    Splitte Antwort in SÃ¤tze (s_j)
    FÃ¼r jeden Satz:
        (align_j = \max_i \cos(\vec e(s_j), \vec e(d_i)))
        (attrib_j = 1) wenn Satz Attributionsmarker hat, sonst 0
    â€œUnsupportedâ€ wenn (align_j < \tau) und (attrib_j=0)

[
U(a)=\frac{#{j:, align_j<\tau \land attrib_j=0}}{n_{\text{sent}}}
]

mit z. B. (\tau = 0.35) (startwert; spÃ¤ter empirisch kalibrieren).


Gesamtformel

[
O_0(a\mid RET)=\mathrm{clip}\left(
\alpha A_{\text{ret}}(a)

    \beta T(a)
    \gamma U(a),
    ,0,1\right)
    ]

Startgewichte (pragmatisch, erklÃ¤rbar):

    (\alpha=0.6) (Grounding ist Kern)
    (\beta=0.2) (Traceability)
    (\gamma=0.2) (Penalty)

4ï¸âƒ£ Semantische ErklÃ¤rung

    (A_{\text{ret}}): â€œIst die Antwort thematisch gestÃ¼tzt?â€
    (T): â€œKann man im Text sehen, worauf sie sich stÃ¼tzt?â€
    (U): â€œWie viel wirkt wie freie Behauptung relativ zum sichtbaren Material?â€

5ï¸âƒ£ Erwartetes Ergebnis

    (O_0 \in [0,1])
    hoch: retrieval-basiert, nachvollziehbar, wenig freie Behauptungen
    niedrig: driftig, unbelegt (im Kontext), viele â€œfreieâ€ SÃ¤tze

6ï¸âƒ£ Vergleichbares erwartetes Ergebnis

Vergleichbar:

    verschiedene Antworten bei gleichem Retrieval-Set
    Modelle bei gleicher Retrieval-Konfiguration
    dieselbe Frage mit anderem Retrieval (zeigt â€œGrounding-QualitÃ¤tâ€ des RAG)

Nicht vergleichbar:

    Antworten mit vÃ¶llig unterschiedlichem Retrieval und ohne Dokumentation des RET
    Setups, in denen Tool-Output im Kontext ist, aber nicht in (\mathcal{R}) einflieÃŸt



_______



ğŸ§© Implementierungsidee (eure n8n/RAG/Python-Strecke)

Inputs:

    a_text
    retrieved_docs[] (id + text)
    optional: tool_outputs[] (falls als Kontext genutzt â†’ dann als zusÃ¤tzliche docs behandeln)

Compute:

    embeddings fÃ¼r a_text und fÃ¼r alle retrieved_docs
    Satzsplit fÃ¼r a_text, Satz-Embeddings fÃ¼r s_j (fÃ¼r U)

Store:

    O0_assistant
    Komponenten: A_ret, T, U
    retrieval_doc_ids
    Parameter: tau, weights, embedding_model_id

Audit ist damit sofort mÃ¶glich.



______


Damit du sicher bist, hier die Developer-Checkliste â€œOâ‚€ in 30 Minuten verstandenâ€:


Oâ‚€ Implementierung â€“ Minimal klar

Inputs (pro Turn)

    a_text (Assistant Output)
    retrieved_docs[] = {id, text} (Top-k, exakt dieses Turn)
    optional: tool_outputs[] wie zusÃ¤tzliche docs (nur wenn sie im Prompt-Kontext waren)
    embedding_model_id, tau, weights

Schritte

    Embeddings

    e_a = embed(a_text)
    e_di = embed(doc_text) fÃ¼r jedes Retrieval-Doc
    Satzsplit a_text -> s_j und e_sj = embed(s_j) (nur fÃ¼r U)

    A_ret (Grounding)

    A_ret = max_i cosine(e_a, e_di)

    T (Traceability)

    Regex-ZÃ¤hler fÃ¼r Attributionsmarker (kleine Wortliste reicht)
    T = clip(n_attrib / (n_sent + 1), 0, 1)

    U (Unsupported Penalty)

    pro Satz:
        align_j = max_i cosine(e_sj, e_di)
        attrib_j = 1 if marker_in_sentence else 0
        unsupported wenn align_j < tau AND attrib_j == 0
    U = unsupported_count / n_sent

    O0

    O0 = clip(alpha*A_ret + beta*T - gamma*U, 0, 1)

Store (fÃ¼r Audit!)

    O0, plus A_ret, T, U
    retrieval_doc_ids
    tau, weights, embedding_model_id

Startwerte

    tau = 0.35
    weights = (0.6, 0.2, 0.2)

Edge Cases

    wenn retrieved_docs leer â†’ O0 nicht berechnen oder A_ret=0 und kennzeichnen
    bei n_sent=0 â†’ T=0, U=0


KRITIK: (und wie man es abfÃ¤ngst):

    â€T ist heuristischâ€œ â†’ ja, bewusst: Transparenz. SpÃ¤ter optional LLM-Classifier, aber Standard-Eval bleibt erklÃ¤rbar.
    â€tau ist willkÃ¼rlichâ€œ â†’ Startwert, wird empirisch kalibriert. tau+weights werden versioniert gespeichert.





ğŸŸ§ KSODI-Full

Operator O â€“ Dynamic Objectivity (Ã¼ber Zeit, Quellen, Checks)

1ï¸âƒ£ Annahmen (zusÃ¤tzlich)

    Sequenzen von Turns verfÃ¼gbar
    optional externe Checks (Web, interne Wissensdatenbanken, Tools)
    Ziel: ObjektivitÃ¤t als StabilitÃ¤t unter PrÃ¼fung
    (Konsistenz, KorrekturfÃ¤higkeit, QuellenqualitÃ¤t, Drift-Resistenz)

2ï¸âƒ£ Beschreibung

O(t) bewertet, ob Aussagen

    konsistent bleiben,
    sich bei Gegenbelegen korrigieren,
    in belastbare Quellen eingebettet sind,
    unter Variation des Kontextes stabil bleiben.

3ï¸âƒ£ Mathematische Formulierung (Full)

Wir nehmen Oâ‚€ als Grundbaustein und ergÃ¤nzen:

    Cross-turn Consistency (C_{\text{turn}}(t))
    Correction Responsiveness (R_{\text{corr}}(t))
    External Verification (V_{\text{ext}}(t)) (optional)

[
O(t)=\mathrm{clip}\Bigl(
w_0 O_0(t)

    w_1 C_{\text{turn}}(t)
    w_2 R_{\text{corr}}(t)
    w_3 V_{\text{ext}}(t)
    \lambda \Delta_{\text{drift}}(t),
    ,0,1\Bigr)
    ]

Semantische ErklÃ¤rung:

    (C_{\text{turn}}): Widerspruchsfreiheit Ã¼ber Zeit
    (R_{\text{corr}}): FÃ¤higkeit zur Selbstkorrektur bei neuen Belegen
    (V_{\text{ext}}): Abgleich mit externen Quellen (wenn erlaubt)
    Drift-Term wie gehabt

4ï¸âƒ£ Erwartetes Ergebnis (Full)

    ObjektivitÃ¤tskurve + â€œPrÃ¼f-Eventsâ€
    sichtbar: robust vs. fragil; korrigiert vs. verteidigt Halluzination

5ï¸âƒ£ Vergleichbarkeit (Full)

Nur in kontrollierten Setups (gleiche PrÃ¼fregeln, gleiche Toolzugriffe).


ğŸ”§ Mini-Review (damit Patrick/Benjamin happy sind)

    Oâ‚€ ist Grounding relativ zu RET, keine â€œWahrheitâ€.
    Nur beobachtbar: Embeddings + Retrieval + Textmarker.
    Parameter (tau, weights) werden gespeichert â†’ reproduzierbar.
    Keine Person, kein Fingerprinting.
